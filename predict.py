# predict.py
# This script loads a pre-trained churn model, scaler, and feature list
# to predict churn probabilities for new, unseen customer data.

# --- Imports ---
import pandas as pd
import numpy as np
import joblib
import os

# Define the cutoff date used in the original analysis for consistency
cutoff_date = pd.to_datetime('2025-03-01')

# --- Feature Engineering Function ---
def create_features(orders_df, contacts_df, product_orders_df):
    """
    Performs the same feature engineering steps as the main analysis script
    to ensure new data is preprocessed identically to the training data.
    """
    # 1. Orders Aggregation
    orders_agg = orders_df.groupby('customerid').agg(
        total_orders=('orderid','nunique'),
        total_quantity=('TotalQty','sum'),
        total_spent=('TotalPrice','sum'),
        first_order_date=('deliveredDate','min'),
        last_order_date=('deliveredDate','max')).reset_index()

    # 2. Contacts Aggregation
    contacts_agg = contacts_df.groupby('customerid').agg(
        num_meetings=('meetingid','nunique'),
        avg_meeting_duration=('MeetingDuration','mean'),
        num_no_activity=('NoActivityReason', lambda x: x.notna().sum())).reset_index()

    # 3. Product Aggregation
    # This section is robust to missing 'customerid' in the product orders file.
    if 'customerid' not in product_orders_df.columns:
        product_orders_df = product_orders_df.merge(orders_df[['orderid', 'customerid']], on='orderid', how='left')

    product_agg = product_orders_df.groupby('customerid').agg(
        total_order_quantity=('Quantity','sum'),
        total_order_price=('Price','sum')).reset_index()

    # 4. Merge Aggregates
    customer_df = orders_agg.merge(contacts_agg, on='customerid', how='left')
    customer_df = customer_df.merge(product_agg, on='customerid', how='left')
    
    # Fill missing numeric values with 0
    numeric_cols_to_fill = ['total_order_quantity', 'total_order_price', 'num_meetings', 'avg_meeting_duration', 'num_no_activity']
    customer_df[numeric_cols_to_fill] = customer_df[numeric_cols_to_fill].fillna(0)

    # 5. Calculate Additional Features
    customer_df['days_since_last_order'] = (cutoff_date - customer_df['last_order_date']).dt.days.fillna(999)
    customer_df['customer_tenure_days'] = (customer_df['last_order_date'] - customer_df['first_order_date']).dt.days.fillna(0)
    customer_df['order_frequency'] = customer_df['total_orders'] / customer_df['customer_tenure_days'].replace(0,1)
    customer_df['avg_order_value'] = customer_df['total_spent'] / customer_df['total_orders'].replace(0,1)
    customer_df['recency_ratio'] = customer_df['days_since_last_order'] / customer_df['customer_tenure_days'].replace(0,1)
    customer_df['meeting_engagement'] = customer_df['num_meetings'] / (customer_df['customer_tenure_days'].replace(0, 1))
    customer_df['brand_loyalty'] = customer_df['total_order_price'] / customer_df['total_spent'].replace(0, 1)

    return customer_df.drop(columns=['first_order_date', 'last_order_date'])

# --- Main Prediction Workflow ---
def predict_churn_for_new_data():
    """
    Loads new data, processes it, and predicts churn probabilities.
    """
    # --- Load Model and Assets ---
    try:
        # Load the trained model, scaler, and the list of columns used for training.
        # These files are generated by the 'customer_churn_analysis.py' script.
        lr_model = joblib.load('models/lr_model.joblib')
        scaler = joblib.load('models/scaler.joblib')
        train_columns = joblib.load('models/train_columns.joblib')
        print("Model, scaler, and training columns loaded successfully.")
    except FileNotFoundError:
        print("Error: Model or scaler files not found. Please ensure they are in the 'models/' directory and that 'customer_churn_analysis.py' has been run.")
        return

    try:
        # Load the raw data from the Excel files
        contacts_raw = pd.read_excel("data/RetentionCaseStudy.xlsx", sheet_name="MeetingData")
        orders = pd.read_excel("data/RetentionCaseStudy-Data2.xlsx", sheet_name="OrderData")
        product_orders = pd.read_excel("data/RetentionCaseStudy-data3.xlsx", sheet_name="Sheet1")
        print("New data files loaded successfully.")
    except FileNotFoundError:
        print("Error: One or more new data files are missing. Please ensure they are in the 'data/' directory.")
        return

    # Data Cleaning (identical to the original script)
    orders['cityname'] = orders['cityname'].fillna('Unknown')
    orders['TownName'] = orders['TownName'].fillna('Unknown')
    orders['OrderStatus'] = orders['OrderStatus'].fillna('Unknown')
    contacts_raw['NoActivityReason'] = contacts_raw['NoActivityReason'].fillna('Active')
    contacts_raw = contacts_raw.dropna(subset=['meetingid'])

    # Create features for the new data
    customer_df_unseen = create_features(orders, contacts_raw, product_orders)
    
    # Separate customer IDs and features
    customer_ids = customer_df_unseen['customerid']
    X_unseen = customer_df_unseen.drop(columns=['customerid'])

    # Get a list of numeric columns for scaling
    numeric_cols = X_unseen.select_dtypes(include=np.number).columns.tolist()

    # One-hot encode categorical features
    categorical_cols = X_unseen.select_dtypes(exclude=np.number).columns.tolist()
    X_unseen = pd.get_dummies(X_unseen, columns=categorical_cols, drop_first=True)

    # Align columns to match the training data
    missing_cols = set(train_columns) - set(X_unseen.columns)
    for c in missing_cols:
        X_unseen[c] = 0
    X_unseen = X_unseen[train_columns]

    # Scale numeric features using the pre-trained scaler
    X_unseen[numeric_cols] = scaler.transform(X_unseen[numeric_cols])

    # Predict churn probabilities
    churn_probabilities = lr_model.predict_proba(X_unseen)[:, 1]

    # Create a DataFrame to display the results
    churn_predictions = pd.DataFrame({
        'customerid': customer_ids,
        'churn_probability': churn_probabilities
    }).drop_duplicates()

    # Sort customers by their churn probability
    top_churn_risks = churn_predictions.sort_values(by='churn_probability', ascending=False)
    
    # Save the final DataFrame to a CSV file
    top_churn_risks.to_csv('top_churn_risks.csv', index=False)
    print("Successfully saved 'top_churn_risks.csv' for Power BI.")

    # Display the results
    print("\n--- Top 10 Customers with Highest Churn Risk ---")
    print(top_churn_risks.head(10).round(4))
    print("\nThis list can be used for targeted retention campaigns.")

# Execute the main function when the script is run
if __name__ == "__main__":
    predict_churn_for_new_data()
